{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# Script for spectral paramaterization and analysis of cleaned and epoched EEG data from\n",
    "# Rygvold T, Hatlestad-Hall C et al. (2021): http://dx.doi.org/10.1111/ejn.14964. SRM Resting-state EEG.\n",
    "# \"DatasetDOI\": \"10.18112/openneuro.ds003775.v1.0.0\"\n",
    "#\n",
    "# Created by Daniel J McKeown, Bond University, 2024.\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as op\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import mne\n",
    "import mne_icalabel\n",
    "from fooof import FOOOFGroup\n",
    "from fooof.bands import Bands\n",
    "from fooof.analysis import get_band_peak_fg\n",
    "from mne.viz import plot_topomap\n",
    "import pingouin as pg\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial import ConvexHull\n",
    "from scipy.stats import pearsonr\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "#Define the home directory where the data is stored\n",
    "home_dir = op.realpath(op.join(op.abspath(''), '..', 'Data')) \n",
    "\n",
    "#Set the monthage,channel information, frequency range and frequency bands\n",
    "montage = mne.channels.make_standard_montage('biosemi64')\n",
    "chan_info = mne.io.read_epochs_eeglab(home_dir + r\"\\info.set\")\n",
    "chan_info = chan_info.info\n",
    "adjacency, _ = mne.channels.find_ch_adjacency(chan_info, ch_type='eeg')\n",
    "freq_range = [1, 50]\n",
    "bands = Bands({'delta': [1,4], 'theta': [4, 8], 'alpha': [8, 13], 'beta': [13, 30], 'gamma': [50, 100]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# Function for spectral parameterisation of EEG data using provided cleaned and epoched data \n",
    "# from Rygvold T, Hatlestad-Hall C et al. (2021)\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to loop through the EEG data and perform FOOOF/SPECPARAM analysis\n",
    "def fooof_eeg_loop(psd_args):\n",
    "\n",
    "    if not 'epochpath' in psd_args:\n",
    "        raise ValueError('Preprocessed path not specified, where is the preprocessed data?')\n",
    "\n",
    "    if not 'psdpath' in psd_args:\n",
    "        raise ValueError('Epoch path not specified, where should the epoched data go?')\n",
    "\n",
    "    epochpath = psd_args['epochpath']\n",
    "    psdpath = psd_args['psdpath']\n",
    "\n",
    "    print(epochpath)\n",
    "    print(psdpath)\n",
    "\n",
    "    # Modify subject_folders to start from the specified startsubj index\n",
    "    subject_folders = [f.name for f in os.scandir(psd_args['epochpath']) if f.is_dir() and f.name.startswith('sub-')]\n",
    "    subject_folders = subject_folders[startsubj:]\n",
    "\n",
    "    for subject_folder in os.listdir(epochpath):\n",
    "        subject_path = op.join(epochpath, subject_folder)\n",
    "        \n",
    "        if not op.isdir(subject_path):\n",
    "            continue\n",
    "\n",
    "        sub_id = subject_folder.split('-')[1]\n",
    "            \n",
    "        for session_folder in ['ses-t1']:\n",
    "            session_path = op.join(subject_path, session_folder)\n",
    "            \n",
    "            if not op.exists(session_path):\n",
    "                continue\n",
    "\n",
    "            eeg_path = op.join(session_path, 'eeg')\n",
    "            \n",
    "            if not op.exists(eeg_path):\n",
    "                continue\n",
    "\n",
    "            set_files = [f for f in os.listdir(eeg_path) if f.endswith('.set')] \n",
    "                \n",
    "            for set_file in set_files:\n",
    "                try:\n",
    "                    set_path = op.join(eeg_path, set_file)\n",
    "                    epoched_eeg_data = mne.read_epochs_eeglab(set_path)\n",
    "                    avg_epochs = epoched_eeg_data.average()\n",
    "                    fmax = 1 / (2 * (1 / avg_epochs.info['sfreq']))\n",
    "                    psd = avg_epochs.compute_psd(method='welch', n_fft=300, n_overlap=150, fmin = 1, fmax = fmax)\n",
    "\n",
    "                    # Extract frequency and power values\n",
    "                    freqs = psd._freqs\n",
    "                    psd = psd._data\n",
    "                    delta_total = psd[:, np.where((freqs >= 1) & (freqs <= 4))[0]].mean(axis=1)\n",
    "                    theta_total = psd[:, np.where((freqs >= 4) & (freqs <= 8))[0]].mean(axis=1)\n",
    "                    alpha_total = psd[:, np.where((freqs >= 8) & (freqs <= 13))[0]].mean(axis=1)\n",
    "                    beta_total = psd[:, np.where((freqs >= 13) & (freqs <= 30))[0]].mean(axis=1)\n",
    "                    gamma_total = psd[:, np.where((freqs >= 50) & (freqs <= 100))[0]].mean(axis=1)\n",
    "                \n",
    "                    # Perform FOOOFGroup analysis\n",
    "                    fg = FOOOFGroup(peak_width_limits=[1.6, 6], min_peak_height=0.05, peak_threshold=1.5, max_n_peaks=8, aperiodic_mode='fixed')\n",
    "                    exps_temp = fg.get_params('aperiodic_params', 'exponent')\n",
    "                    offset_temp = fg.get_params('aperiodic_params', 'offset')\n",
    "                    peaks_temp = list(fg.get_params('peak_params'))\n",
    "                    r_squ_temp = fg.get_params('r_squared')\n",
    "                    delta_fooof = get_band_peak_fg(fg, bands['delta'])\n",
    "                    theta_fooof = get_band_peak_fg(fg, bands['theta'])\n",
    "                    alpha_fooof = get_band_peak_fg(fg, bands['alpha'])\n",
    "                    beta_fooof = get_band_peak_fg(fg, bands['beta'])\n",
    "                    gamma_fooof = get_band_peak_fg(fg, bands['gamma'])\n",
    "\n",
    "                    # Store results in the dictionary\n",
    "                    results['ID'].append(sub_id)\n",
    "                    results['exponent'].append(exps_temp)\n",
    "                    results['offset'].append(offset_temp)\n",
    "                    results['peaks'].append(peaks_temp)\n",
    "                    results['r2'].append(r_squ_temp)\n",
    "                    results['total']['delta'].append(delta_total)\n",
    "                    results['total']['theta'].append(theta_total)\n",
    "                    results['total']['alpha'].append(alpha_total)\n",
    "                    results['total']['beta'].append(beta_total)\n",
    "                    results['total']['gamma'].append(gamma_total)\n",
    "                    results['fooof']['delta'].append(delta_fooof)\n",
    "                    results['fooof']['theta'].append(theta_fooof)\n",
    "                    results['fooof']['alpha'].append(alpha_fooof)\n",
    "                    results['fooof']['beta'].append(beta_fooof)\n",
    "                    results['fooof']['gamma'].append(gamma_fooof)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing Subject: {subject_folder}, Session: {session_folder}, File: {fif_file}\")\n",
    "                    print(f\"Error message: {str(e)}\")\n",
    "                    continue\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# Other functions for analysis\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions for creating and manipulating dictionaries of fits#\n",
    "def bad_fits(fit):\n",
    "    \"\"\"\n",
    "    Return the number of fits below .90.\n",
    "    \"\"\"\n",
    "    return np.sum(fit < .9)\n",
    "\n",
    "def bad_fits_mask(fit):\n",
    "    \"\"\"\n",
    "    Create a mask for the channels that have fits below .90.\n",
    "    \"\"\"\n",
    "    return np.array(fit) < .9\n",
    "  \n",
    "def replace_bad_fits(fit, mask):\n",
    "    \"\"\"\n",
    "    Replace bad fits in each dictionary entry with nans using the mask function.\n",
    "    \"\"\"\n",
    "    return np.where(mask, np.nan, fit)\n",
    "\n",
    "def replace_bad_fits_all(fit, mask):\n",
    "    \"\"\"\n",
    "    Make the above into a function\n",
    "    \"\"\"\n",
    "    fit['r2'] = replace_bad_fits(fit['r2'], mask)\n",
    "    return fit\n",
    "\n",
    "\n",
    "#Function for replacing nans with the mean or zero\n",
    "def replace_nans(data, nan_policy='mean', min_valid_percentage=0):\n",
    "    \"\"\"\n",
    "    Replace NaN values in a pandas Series or NumPy array with either the mean or zero.\n",
    "\n",
    "    Parameters:\n",
    "    data (Series or array-like): The input pandas Series or NumPy array.\n",
    "    nan_policy (str): The policy to replace NaNs. Options are 'mean' or 'zero'.\n",
    "    min_valid_percentage (float): The minimum valid percentage required for replacing NaNs.\n",
    "\n",
    "    Returns:\n",
    "    Series or array-like: Data with NaNs replaced.\n",
    "    \"\"\"\n",
    "    if isinstance(data, pd.Series):\n",
    "        non_nan_count = data.notna().sum()\n",
    "        total_elements = data.shape[0]\n",
    "    elif isinstance(data, np.ndarray):\n",
    "        non_nan_count = np.count_nonzero(~np.isnan(data))\n",
    "        total_elements = data.size\n",
    "    else:\n",
    "        raise ValueError(\"Input must be a pandas Series or NumPy array.\")\n",
    "\n",
    "    non_nan_percentage = non_nan_count / total_elements\n",
    "\n",
    "    if non_nan_percentage >= (min_valid_percentage / 100):\n",
    "        if nan_policy == 'mean':\n",
    "            if isinstance(data, pd.Series):\n",
    "                return data.fillna(data.mean())\n",
    "            elif isinstance(data, np.ndarray):\n",
    "                mean_value = np.nanmean(data)\n",
    "                return np.where(np.isnan(data), mean_value, data)\n",
    "        elif nan_policy == 'zero':\n",
    "            if isinstance(data, pd.Series):\n",
    "                return data.fillna(0)\n",
    "            elif isinstance(data, np.ndarray):\n",
    "                return np.nan_to_num(data, nan=0)\n",
    "        else:\n",
    "            raise ValueError('Nan policy not understood.')\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "\n",
    "#Function for adding electrode prefix to variable names in DataFrame\n",
    "def add_electrode_prefix(data, chan_info, variable_name):\n",
    "    \"\"\"\n",
    "    Add electrode prefix to variable names in DataFrame.\n",
    "    \"\"\"\n",
    "    for column in data.columns[1:]:\n",
    "        electrode_names = chan_info['ch_names']\n",
    "        new_column_names = [f\"{electrode}_{variable_name}\" for electrode in electrode_names]\n",
    "        data.rename(columns={column: new_column_names[data.columns.get_loc(column) - 1]}, inplace=True)\n",
    "    return data\n",
    "\n",
    "\n",
    "#Function for plotting clusters on topomap\n",
    "def plot_cluster_topomap(mask, cluster_name, cmap):\n",
    "    \"\"\"\n",
    "    Function to plot topomap for a cluster with a specific color map.\n",
    "    \"\"\"\n",
    "    cluster_electrodes = np.array(chan_names)[mask]\n",
    "    cluster_positions = {elec: positions[elec] for elec in cluster_electrodes if elec in positions}\n",
    "    data = np.zeros(len(chan_names))\n",
    "    data[mask] = 1\n",
    "    pos = np.array([positions[elec][:2] for elec in chan_names if elec in positions])\n",
    "    info = mne.create_info(ch_names=chan_names, sfreq=1000, ch_types=\"eeg\")\n",
    "    info.set_montage(montage)\n",
    "    fig, ax = plt.subplots()\n",
    "    mne.viz.plot_topomap(data, pos=pos, names=chan_names, axes=ax, contours=0, size=15, cmap=cmap, extrapolate='box')\n",
    "    ax.set_title(cluster_name)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "#Class for KMeans Clustering\n",
    "class KMeansClustering:\n",
    "\n",
    "    def __init__(self, k=6):\n",
    "        self.k = k\n",
    "        self.centroids = None\n",
    "\n",
    "    @staticmethod\n",
    "    def euclidean_distance(data_point, centroids):\n",
    "        return np.sqrt(np.sum((centroids - data_point)**2, axis=1))\n",
    "\n",
    "    def fit(self, X, max_iterations=400, random_state=None):\n",
    "        if random_state is not None:\n",
    "            np.random.seed(random_state)\n",
    "        \n",
    "        self.centroids = np.random.uniform(np.amin(X, axis=0), np.amax(X, axis=0),\n",
    "            size=(self.k, X.shape[1]))\n",
    "        \n",
    "        for _ in range(max_iterations):\n",
    "            y = []\n",
    "\n",
    "            for data_point in X:\n",
    "                distances = KMeansClustering.euclidean_distance(data_point, self.centroids)\n",
    "                cluster_num = np.argmin(distances)\n",
    "                y.append(cluster_num)\n",
    "\n",
    "            y = np.array(y)\n",
    "\n",
    "            cluster_indices =[]\n",
    "\n",
    "            for i in range(self.k):\n",
    "                cluster_indices.append(np.argwhere(y ==i))\n",
    "\n",
    "            cluster_centers = []\n",
    "\n",
    "            for i, indices in enumerate(cluster_indices):\n",
    "                if len(indices) ==0:\n",
    "                    cluster_centers.append(self.centroids[i])\n",
    "                else:\n",
    "                    cluster_centers.append(np.mean(X[indices], axis=0)[0])\n",
    "\n",
    "            if np.max(self.centroids - np.array(cluster_centers)) < 0.0001:\n",
    "                break\n",
    "            else:\n",
    "                self.centroids = np.array(cluster_centers)\n",
    "\n",
    "        return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dictionary to store the results\n",
    "results = {\n",
    "    'ID': [],\n",
    "    'exponent': [],\n",
    "    'offset': [],\n",
    "    'peaks': [],\n",
    "    'r2': [],\n",
    "    'total': {\n",
    "        'delta': [],\n",
    "        'theta': [],\n",
    "        'alpha': [],\n",
    "        'beta': [],\n",
    "        'gamma': []\n",
    "    },\n",
    "    'fooof': {\n",
    "        'delta': [],\n",
    "        'theta': [],\n",
    "        'alpha': [],\n",
    "        'beta': [],\n",
    "        'gamma': []\n",
    "    }\n",
    "}\n",
    "\n",
    "#Args for parameterization\n",
    "#If you have got the data from Rygvold T, Hatlestad-Hall C et al. (2021): http://dx.doi.org/10.1111/ejn.14964. SRM Resting-state EEG. Will be organised in this BIDS structure\n",
    "#Create a folder to store the results of the FOOOF analysis\n",
    "psd_args = {\n",
    "    'epochpath': home_dir + r'\\derivatives\\cleaned_data', \n",
    "    'psdpath': home_dir + r'\\derivatives\\fooof_data', \n",
    "}\n",
    "\n",
    "#Set to None to process all subjects, or specify a subject if needed\n",
    "subject = None  \n",
    "# Set the starting subject index\n",
    "startsubj = 0 \n",
    "#List the subjects in the sourcepath with the prefix sub-\n",
    "subject_folders = [f.name for f in os.scandir(psd_args['psdpath']) if f.is_dir() and f.name.startswith('sub-')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you want to start parameterization from a specific subject, you can use the following code\n",
    "subject_identifier = 'sub-020'\n",
    "try:\n",
    "    startsubj_index = subject_folders.index(subject_identifier)\n",
    "    print(f\"The index for {subject_identifier} is: {startsubj_index}\")\n",
    "except ValueError:\n",
    "    print(f\"{subject_identifier} is not found in the subject folders.\")\n",
    "    #Set a default index to handle the case as needed\n",
    "    startsubj_index = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the function to perform the FOOOF analysis\n",
    "results = fooof_eeg_loop(psd_args)\n",
    "\n",
    "# Save the results to npy files\n",
    "results = results[0]\n",
    "np.save(home_dir + r'\\results.npy', results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# Data quality and reduction\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in the results of the parameterisation and demographic data provided by Hatlestad-Hall C et al. (2021)\n",
    "results = np.load(home_dir + r\"\\results.npy\", allow_pickle=True).tolist()\n",
    "demo_data = pd.read_csv(home_dir + r\"\\demo_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How well does the FOOOF model fit the data?\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "vlim = (.9, 1)\n",
    "im, cm = plot_topomap(np.mean(np.array(results['r2']), axis=0), chan_info, size=5, cmap='jet', show=False, contours=10, image_interp='cubic', axes=ax, vlim=vlim)\n",
    "cbar_ax = fig.add_axes([0.15, 0.01, 0.72, 0.05])\n",
    "cbar = plt.colorbar(im, cax=cbar_ax, orientation='horizontal')\n",
    "cbar.set_label('SpecParam fit (R2)', fontsize=12)\n",
    "cbar.ax.tick_params(labelsize=12)\n",
    "fig.set_size_inches(10,3)\n",
    "fig.subplots_adjust(wspace=0.05, hspace=0.2)\n",
    "\n",
    "#what is the average r2 value across all participants and all channels?\n",
    "print( 'average r2:', np.mean(np.mean(np.array(results['r2']), axis=0)))\n",
    "print( 'std r2:', np.std(np.std(np.array(results['r2']), axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace bad fits with NaNs\n",
    "results = replace_bad_fits_all(results, bad_fits_mask(results['r2']))\n",
    "\n",
    "#Which participants have more than 50% of their channels with bad fits?_nan_mask = np.sum(np.isnan(results['r2']).squeeze(),axis=1)/64 > .5\n",
    "nan_mask  = np.sum(np.isnan(results['r2']).squeeze(),axis=1)/64 > .5\n",
    "\n",
    "#Reverse the mask to get the good fits_good_mask = _nan_mask\n",
    "good_mask = ~nan_mask\n",
    "\n",
    "#How many participants have more than 50% of their channels with bad fits?\n",
    "print(\"Participants with >50% bad fits:\",np.sum(nan_mask))\n",
    "\n",
    "#Which IDs are in the good fits?_good_id = np.array(results['ID'])_good_mask].astype(int)\n",
    "good_id = np.array(results['ID'])[good_mask].astype(int)\n",
    "\n",
    "#How many IDs are in the good fits?\n",
    "print(\"Good participants:\", good_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the number of NaNs in each measure for each participant in each group per electrode location\n",
    "nan_count_exponent = np.sum(\n",
    "    np.isnan(np.array(results['exponent']).squeeze()), axis=1)\n",
    "nan_count_offset = np.sum(\n",
    "    np.isnan(np.array(results['offset']).squeeze()), axis=1)\n",
    "\n",
    "nan_count_total_delta_pw = np.sum(\n",
    "    np.isnan(np.array(results['total']['delta']).squeeze()), axis=1)\n",
    "nan_count_total_theta_pw = np.sum(\n",
    "    np.isnan(np.array(results['total']['theta']).squeeze()), axis=1)\n",
    "nan_count_total_alpha_pw = np.sum(\n",
    "    np.isnan(np.array(results['total']['alpha']).squeeze()), axis=1)\n",
    "nan_count_total_beta_pw = np.sum(\n",
    "    np.isnan(np.array(results['total']['beta']).squeeze()), axis=1)\n",
    "nan_count_total_gamma_pw = np.sum(\n",
    "    np.isnan(np.array(results['total']['gamma']).squeeze()), axis=1)\n",
    "\n",
    "nan_count_fooof_delta_pw = np.sum(\n",
    "    np.isnan(np.array(results['fooof']['delta'])[:,:,0].squeeze()), axis=1)\n",
    "nan_count_fooof_theta_pw = np.sum(\n",
    "    np.isnan(np.array(results['fooof']['theta'])[:,:,0].squeeze()), axis=1)\n",
    "nan_count_fooof_alpha_pw = np.sum(\n",
    "    np.isnan(np.array(results['fooof']['alpha'])[:,:,0].squeeze()), axis=1)\n",
    "nan_count_fooof_beta_pw = np.sum(\n",
    "    np.isnan(np.array(results['fooof']['beta'])[:,:,0].squeeze()), axis=1)\n",
    "nan_count_fooof_gamma_pw = np.sum(\n",
    "    np.isnan(np.array(results['fooof']['gamma'])[:,:,0].squeeze()), axis=1)\n",
    "\n",
    "#What is the proportion of retained channels for each measure?\n",
    "prop_exponent = 100 - (nan_count_exponent.mean()/64)*100\n",
    "prop_offset = 100 - (nan_count_offset.mean()/64)*100\n",
    "prop_total_delta_pw = 100 - (nan_count_total_delta_pw.mean()/64)*100\n",
    "prop_total_theta_pw = 100 - (nan_count_total_theta_pw.mean()/64)*100\n",
    "prop_total_alpha_pw = 100 - (nan_count_total_alpha_pw.mean()/64)*100\n",
    "prop_total_beta_pw = 100 - (nan_count_total_beta_pw.mean()/64)*100\n",
    "prop_total_gamma_pw = 100 - (nan_count_total_gamma_pw.mean()/64)*100\n",
    "prop_fooof_delta_pw = 100 - (nan_count_fooof_delta_pw.mean()/64)*100\n",
    "prop_fooof_theta_pw = 100 - (nan_count_fooof_theta_pw.mean()/64)*100\n",
    "prop_fooof_alpha_pw = 100 - (nan_count_fooof_alpha_pw.mean()/64)*100\n",
    "prop_fooof_beta_pw = 100 - (nan_count_fooof_beta_pw.mean()/64)*100\n",
    "prop_fooof_gamma_pw = 100 - (nan_count_fooof_gamma_pw.mean()/64)*100\n",
    "\n",
    "print(\"Proportion of retained exponent in:\" , prop_exponent)\n",
    "print(\"Proportion of retained offset in:\" , prop_offset)\n",
    "print(\"Proportion of retained total delta pw in:\" , prop_total_delta_pw)\n",
    "print(\"Proportion of retained total theta pw in:\" , prop_total_theta_pw)\n",
    "print(\"Proportion of retained total alpha pw in:\" , prop_total_alpha_pw)\n",
    "print(\"Proportion of retained total beta pw in:\" , prop_total_beta_pw)\n",
    "print(\"Proportion of retained total gamma pw in:\" , prop_total_gamma_pw)\n",
    "print(\"Proportion of retained fooof delta pw in:\" , prop_fooof_delta_pw)\n",
    "print(\"Proportion of retained fooof theta pw in:\" , prop_fooof_theta_pw)\n",
    "print(\"Proportion of retained fooof alpha pw in:\" , prop_fooof_alpha_pw)\n",
    "print(\"Proportion of retained fooof beta pw in:\" , prop_fooof_beta_pw)\n",
    "print(\"Proportion of retained fooof gamma pw in:\" , prop_fooof_gamma_pw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do any participants have scalp wide mean exponent values that are greater than +- 3SD from the mean?\n",
    "\n",
    "exponent = np.mean(np.array(results['exponent'])[good_mask].squeeze(), axis=1)\n",
    "exponent = exponent[~np.isnan(exponent)]\n",
    "exponent_mean = np.mean(exponent)\n",
    "exponent_sd = np.std(exponent)\n",
    "exponent_outliers = np.abs(exponent - exponent_mean) > 3*exponent_sd\n",
    "exponent_outliers_id = good_id[exponent_outliers]\n",
    "print(\"Outliers in exponent:\", exponent_outliers_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot exponent on a topomap\n",
    "vlim = (.7, 1.7)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "im, cm = plot_topomap(np.mean(np.array(results['exponent'])[good_mask], axis=0), chan_info, size=5, cmap='jet', show=False, contours=10, image_interp='cubic', axes=ax, vlim=vlim)\n",
    "cbar_ax = fig.add_axes([0.15, 0.01, 0.72, 0.05])\n",
    "cbar = plt.colorbar(im, cax=cbar_ax, orientation='horizontal')\n",
    "cbar.set_label('Exponent (a.u.)', fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot offset on a topomap\n",
    "vlim = (-13.5, -12.5)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "im, cm = plot_topomap(np.mean(np.array(results['offset'])[good_mask], axis=0), chan_info, size=5, cmap='jet', show=False, contours=10, image_interp='cubic', axes=ax, vlim=vlim)\n",
    "cbar_ax = fig.add_axes([0.15, 0.01, 0.72, 0.05])\n",
    "cbar = plt.colorbar(im, cax=cbar_ax, orientation='horizontal')\n",
    "cbar.set_label('Offset (Hz)', fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at correlations of exponent and offset across the scalp\n",
    "exponent_data = np.array(results['exponent'])\n",
    "exponent_corr = np.corrcoef(exponent_data, rowvar=False)\n",
    "plt.figure(figsize=(10, 10))\n",
    "heatmap = sns.heatmap(exponent_corr, cmap='RdBu', annot=False, fmt=\".2f\", cbar_kws={'label': ''}, xticklabels=chan_info['ch_names'], yticklabels=chan_info['ch_names'], vmin = -1, vmax = 1)\n",
    "cbar = heatmap.collections[0].colorbar\n",
    "cbar.ax.tick_params(labelsize=16)\n",
    "plt.title('')\n",
    "plt.show()\n",
    "\n",
    "offset_data = np.array(results['offset'])\n",
    "offset_corr = np.corrcoef(offset_data, rowvar=False)\n",
    "plt.figure(figsize=(10, 10))\n",
    "heatmap = sns.heatmap(offset_corr, cmap='RdBu', annot=False, fmt=\".2f\", cbar_kws={'label': ''}, xticklabels=chan_info['ch_names'], yticklabels=chan_info['ch_names'], vmin = -1, vmax = 1)\n",
    "plt.title('')\n",
    "cbar = heatmap.collections[0].colorbar\n",
    "cbar.ax.tick_params(labelsize=16)\n",
    "plt.show()\n",
    "\n",
    "#what are the average correlations values for the exponent_corr in the frontal and parietal locations?\n",
    "frontal = ['Fp1', 'Fp2', 'F3', 'F4', 'F7', 'F8', 'Fz']\n",
    "parietal = ['P3', 'P4', 'P7', 'P8', 'Pz', 'PO3', 'PO4', 'PO7', 'PO8', 'POz', 'O1', 'O2']\n",
    "\n",
    "frontal_inds = [chan_info['ch_names'].index(ch) for ch in frontal]\n",
    "parietal_inds = [chan_info['ch_names'].index(ch) for ch in parietal]\n",
    "\n",
    "frontal_corr = np.mean(offset_corr[frontal_inds][:, frontal_inds])\n",
    "parietal_corr = np.mean(offset_corr[parietal_inds][:, parietal_inds])\n",
    "\n",
    "print(\"Frontal correlation:\", frontal_corr)\n",
    "print(\"Parietal correlation:\", parietal_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform PCA on the exponent and offset data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data reduction for exponent\n",
    "pca = PCA(n_components=10)\n",
    "\n",
    "#fit the PCA model\n",
    "exp_pca = pca.fit(exponent_data)\n",
    "exp_explained_variance = pca.explained_variance_ratio_\n",
    "exp_cumulative_explained_variance = np.sum(exp_explained_variance)\n",
    "\n",
    "#plot the explained variance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(exp_explained_variance) + 1), exp_explained_variance, marker='o', linestyle='-')\n",
    "plt.title('Exponent')\n",
    "plt.xlabel('Principal Components', fontsize = 16)\n",
    "plt.ylabel('Eigenvalue', fontsize = 16)\n",
    "plt.xticks(range(1, len(exp_explained_variance) + 1), fontsize = 16)\n",
    "plt.yticks(fontsize = 16)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print('Explained variance:', exp_explained_variance)\n",
    "exp_loadings = pca.components_\n",
    "\n",
    "# Plot the loadings for the PC1 component\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(exp_loadings[0])), exp_loadings[0], align='center', alpha=0.5)\n",
    "plt.title('Exponent')\n",
    "plt.xlabel('Electrode', fontsize = 16)\n",
    "plt.ylabel('PC1 Loadings', fontsize = 16)\n",
    "plt.xticklabels = chan_info['ch_names']\n",
    "plt.xticks(fontsize = 16)\n",
    "plt.yticks(fontsize = 16)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "#plot the loadings for the PC2 component\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(exp_loadings[1])), exp_loadings[1], align='center', alpha=0.5)\n",
    "plt.title('Exponent')\n",
    "plt.xlabel('Electrode', fontsize = 16)\n",
    "plt.ylabel('PC2 Loadings', fontsize = 16)\n",
    "plt.xticklabels = chan_info['ch_names']\n",
    "plt.xticks(fontsize = 16)\n",
    "plt.yticks(fontsize = 16)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "##Perform k-means clustering\n",
    "\n",
    "exponent_data_transposed = exponent_data.T\n",
    "\n",
    "# Define a range of values for k (number of clusters) to try\n",
    "k_values = range(2, 11)\n",
    "\n",
    "# Initialize lists to store silhouette scores and inertia\n",
    "inertia_values = []\n",
    "silhouette_scores = []\n",
    "\n",
    "# Iterate over each value of k\n",
    "for k in k_values:\n",
    "    # Fit KMeans clustering model\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    labels = kmeans.fit_predict(exponent_data_transposed)\n",
    "    # Compute inertia\n",
    "    inertia_values.append(kmeans.inertia_)\n",
    "    # Compute silhouette score\n",
    "    silhouette_avg = silhouette_score(exponent_data_transposed, labels)\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "# Plot elbow plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(k_values, inertia_values, marker='o')\n",
    "plt.xlabel('Number of Clusters (k)', fontsize = 16)\n",
    "plt.ylabel('Within-Cluster Variance', fontsize = 16)\n",
    "plt.title('Exponent Elbow Plot')\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot silhouette scores\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(k_values, silhouette_scores, marker='o')\n",
    "plt.xlabel('Number of Clusters (k)', fontsize = 16)\n",
    "plt.ylabel('Silhouette Score', fontsize = 16)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.title('Silhouette Score for Different Values of k')\n",
    "\n",
    "# Fit KMeans clustering model\n",
    "kmeans = KMeansClustering(k=4)\n",
    "labels = kmeans.fit(exponent_data_transposed, random_state=42)\n",
    "\n",
    "#plot the data in clusters in the dimensions of pca 1 and pca2\n",
    "# Fit PCA to your transposed exponent data\n",
    "pca = PCA(n_components=2)\n",
    "exponent_pca = pca.fit_transform(exponent_data_transposed)\n",
    "\n",
    "# Create a scatter plot of the transformed data points, colored by cluster labels\n",
    "plt.figure(figsize=(10, 8))\n",
    "for cluster_num in np.unique(labels):\n",
    "    cluster_points = exponent_pca[labels == cluster_num]\n",
    "    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], label=f'Cluster {cluster_num}', alpha=0.7, s=50)\n",
    "\n",
    "    for i, (x, y) in enumerate(zip(cluster_points[:, 0], cluster_points[:, 1])):\n",
    "        # Calculate the index of the electrode name in the list\n",
    "        idx = np.where((exponent_pca[:, 0] == x) & (exponent_pca[:, 1] == y))[0][0]\n",
    "        plt.text(x, y, chan_info['ch_names'][idx], fontsize=12, ha='left', va='bottom')\n",
    "\n",
    "# Plot convex hull for each cluster\n",
    "cluster_centers = kmeans.centroids\n",
    "for i in range(len(cluster_centers)):\n",
    "    cluster_points = exponent_pca[labels == i]\n",
    "    hull = ConvexHull(cluster_points)\n",
    "\n",
    "    plt.plot(np.append(cluster_points[hull.vertices, 0], cluster_points[hull.vertices[0], 0]),\n",
    "             np.append(cluster_points[hull.vertices, 1], cluster_points[hull.vertices[0], 1]), \n",
    "             lw=2)\n",
    "    plt.fill(np.append(cluster_points[hull.vertices, 0], cluster_points[hull.vertices[0], 0]), \n",
    "             np.append(cluster_points[hull.vertices, 1], cluster_points[hull.vertices[0], 1]), \n",
    "             alpha=0.1)  # Shade area\n",
    "\n",
    "plt.xlabel('Dimension 1 (31.9%)', fontsize=16)\n",
    "plt.ylabel('Dimension 2 (14.7%)', fontsize=16)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.title('Exponent KMeans Clustering', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "#create a mask for the exponent values for each cluster\n",
    "occipital_mask = labels == 0\n",
    "frontoparietal_mask = labels == 1\n",
    "midline_mask = labels == 2\n",
    "central_mask = labels == 3\n",
    "\n",
    "#calculate the mean exponent value for each cluster\n",
    "occipital_exponent = np.mean(exponent_data_transposed[occipital_mask], axis=0)\n",
    "frontoparietal_exponent = np.mean(exponent_data_transposed[frontoparietal_mask], axis=0)\n",
    "midline_exponent = np.mean(exponent_data_transposed[midline_mask], axis=0)\n",
    "central_exponent = np.mean(exponent_data_transposed[central_mask], axis=0)\n",
    "\n",
    "#pull out the alpha peak freq data for each the exponent clusters\n",
    "fooof_alpha_freq_data = np.array(results['fooof']['alpha'])[:,:,0].squeeze()\n",
    "fooof_alpha_freq_data = fooof_alpha_freq_data.T\n",
    "occipital_exp_fooof_alpha_freq = np.mean(fooof_alpha_freq_data[occipital_mask], axis=0)\n",
    "frontoparietal_exp_fooof_alpha_freq = np.mean(fooof_alpha_freq_data[frontoparietal_mask], axis=0)\n",
    "midline_exp_fooof_alpha_freq = np.mean(fooof_alpha_freq_data[midline_mask], axis=0)\n",
    "central_exp_fooof_alpha_freq = np.mean(fooof_alpha_freq_data[central_mask], axis=0)\n",
    "\n",
    "#Visualise the exponent clusters\n",
    "chan_names = chan_info['ch_names']\n",
    "positions = montage.get_positions()['ch_pos']\n",
    "positions = {chan: pos for chan, pos in positions.items() if chan in chan_names}\n",
    "\n",
    "colormaps = {\n",
    "    'Occipital Cluster': 'Blues',\n",
    "    'Frontoparietal Cluster': 'Oranges',\n",
    "    'Midline Cluster': 'Greens',\n",
    "    'Central Cluster': 'Reds'\n",
    "}\n",
    "\n",
    "plot_cluster_topomap(occipital_mask, 'Occipital Cluster', colormaps['Occipital Cluster'])\n",
    "plot_cluster_topomap(frontoparietal_mask, 'Frontoparietal Cluster', colormaps['Frontoparietal Cluster'])\n",
    "plot_cluster_topomap(midline_mask, 'Midline Cluster', colormaps['Midline Cluster'])\n",
    "plot_cluster_topomap(central_mask, 'Central Cluster', colormaps['Central Cluster'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data reduction for offset\n",
    "\n",
    "#fit the PCA model\n",
    "off_pca = pca.fit(offset_data)\n",
    "off_explained_variance = pca.explained_variance_ratio_\n",
    "off_cumulative_explained_variance = np.cumsum(off_explained_variance)\n",
    "\n",
    "#plot the explained variance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(off_explained_variance) + 1), off_explained_variance, marker='o', linestyle='-')\n",
    "plt.title('Offset')\n",
    "plt.xlabel('Principal Components', fontsize =16)\n",
    "plt.ylabel('Eigenvalue', fontsize = 16)\n",
    "plt.xticks(fontsize = 16)\n",
    "plt.yticks(fontsize = 16)\n",
    "plt.xticks(range(1, len(off_explained_variance) + 1))\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print('Explained variance:', off_explained_variance)\n",
    "\n",
    "# Plot the loadings for the PC1 component\n",
    "off_loadings = pca.components_\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(off_loadings[0])), off_loadings[0], align='center', alpha=0.5)\n",
    "plt.title('Offset')\n",
    "plt.xlabel('Electrode', fontsize = 16)\n",
    "plt.ylabel('PC1 Loadings', fontsize = 16)\n",
    "plt.xticklabels = chan_info['ch_names']\n",
    "plt.xticks(fontsize = 16)\n",
    "plt.yticks(fontsize = 16)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "#plot the loadings for the PC2 component\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(off_loadings[1])), off_loadings[1], align='center', alpha=0.5)\n",
    "plt.title('Offset')\n",
    "plt.xlabel('Electrode', fontsize = 16)\n",
    "plt.ylabel('PC2 Loadings', fontsize = 16)\n",
    "plt.xticklabels = chan_info['ch_names']\n",
    "plt.xticks(fontsize = 16)\n",
    "plt.yticks(fontsize = 16)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "#Perform k-means clustering for offset\n",
    "offset_data_transposed = offset_data.T\n",
    "k_values = range(2, 11)\n",
    "inertia_values = []\n",
    "silhouette_scores = []\n",
    "\n",
    "# Iterate over each value of k\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    labels = kmeans.fit_predict(offset_data_transposed)\n",
    "    inertia_values.append(kmeans.inertia_)\n",
    "    silhouette_avg = silhouette_score(offset_data_transposed, labels)\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "# Plot elbow plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(k_values, inertia_values, marker='o')\n",
    "plt.xlabel('Number of Clusters (k)', fontsize = 16)\n",
    "plt.ylabel('Within-Cluster Variance', fontsize = 16)\n",
    "plt.xticks(fontsize = 16)\n",
    "plt.yticks(fontsize = 16)\n",
    "plt.title('Offset Elbow Plot')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot silhouette scores\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(k_values, silhouette_scores, marker='o')\n",
    "plt.xlabel('Number of Clusters (k)', fontsize = 16)\n",
    "plt.ylabel('Silhouette Score', fontsize = 16)\n",
    "plt.xticks(fontsize = 16)\n",
    "plt.yticks(fontsize = 16)\n",
    "plt.title('Silhouette Score for Different Values of k')\n",
    "\n",
    "\n",
    "# Fit KMeans clustering model\n",
    "kmeans = KMeansClustering(k=5)\n",
    "labels = kmeans.fit(offset_data_transposed, random_state=42)\n",
    "\n",
    "#plot the data in clusters in the dimensions of pca 1 and pca2\n",
    "# Fit PCA to your transposed exponent data\n",
    "pca = PCA(n_components=2)\n",
    "offset_pca = pca.fit_transform(offset_data_transposed)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "for cluster_num in np.unique(labels):\n",
    "    cluster_points = offset_pca[labels == cluster_num]\n",
    "    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], label=f'Cluster {cluster_num}', alpha=0.7, s=50)\n",
    "\n",
    "    for i, (x, y) in enumerate(zip(cluster_points[:, 0], cluster_points[:, 1])):\n",
    "        # Calculate the index of the electrode name in the list\n",
    "        idx = np.where((offset_pca[:, 0] == x) & (offset_pca[:, 1] == y))[0][0]\n",
    "        plt.text(x, y, chan_info['ch_names'][idx], fontsize=12, ha='left', va='bottom')\n",
    "\n",
    "\n",
    "# Plot convex hull for each cluster\n",
    "cluster_centers = kmeans.centroids\n",
    "\n",
    "for i in range(len(cluster_centers)):\n",
    "    cluster_points = offset_pca[labels == i]\n",
    "    \n",
    "    if len(cluster_points) > 0:\n",
    "        hull = ConvexHull(cluster_points)\n",
    "        \n",
    "        plt.plot(np.append(cluster_points[hull.vertices, 0], cluster_points[hull.vertices[0], 0]),\n",
    "                 np.append(cluster_points[hull.vertices, 1], cluster_points[hull.vertices[0], 1]), \n",
    "                 lw=2)\n",
    "        \n",
    "        plt.fill(np.append(cluster_points[hull.vertices, 0], cluster_points[hull.vertices[0], 0]), \n",
    "                 np.append(cluster_points[hull.vertices, 1], cluster_points[hull.vertices[0], 1]), \n",
    "                 alpha=0.1)  # Shade area\n",
    "\n",
    "plt.xlabel('Dimension 1 (21.2%)', fontsize=16)\n",
    "plt.ylabel('Dimension 2 (9.4%)', fontsize=16)\n",
    "plt.title('Offset KMeans Clustering', fontsize=16)\n",
    "plt.xticks(fontsize = 16)\n",
    "plt.yticks(fontsize = 16)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#create a mask for the offset values for each cluster\n",
    "occipital_mask = labels == 4\n",
    "frontoparietal_mask = labels == 1\n",
    "central_mask = labels == 0\n",
    "\n",
    "#calculate the mean offset value for each cluster\n",
    "occipital_offset = np.mean(offset_data_transposed[occipital_mask], axis=0)\n",
    "frontoparietal_offset = np.mean(offset_data_transposed[frontoparietal_mask], axis=0)\n",
    "central_offset = np.mean(offset_data_transposed[central_mask], axis=0)\n",
    "fooof_alpha_freq_data = np.array(results['fooof']['alpha'])[:,:,0].squeeze()\n",
    "fooof_alpha_freq_data = fooof_alpha_freq_data.T\n",
    "occipital_off_fooof_alpha_freq = np.mean(fooof_alpha_freq_data[occipital_mask], axis=0)\n",
    "frontoparietal_off_fooof_alpha_freq = np.mean(fooof_alpha_freq_data[frontoparietal_mask], axis=0)\n",
    "central_off_fooof_alpha_freq = np.mean(fooof_alpha_freq_data[central_mask], axis=0)\n",
    "\n",
    "#Visualise the offset clusters\n",
    "positions = {chan: pos for chan, pos in positions.items() if chan in chan_names}\n",
    "\n",
    "# Define colormaps for each cluster\n",
    "colormaps = {\n",
    "    'Occipital Cluster': 'Greens',\n",
    "    'Frontoparietal Cluster': 'Oranges',\n",
    "    'Central Cluster': 'Blues'\n",
    "}\n",
    "\n",
    "# Plotting each cluster with its specified colormap\n",
    "plot_cluster_topomap(occipital_mask, 'Occipital Cluster', colormaps['Occipital Cluster'])\n",
    "plot_cluster_topomap(frontoparietal_mask, 'Frontoparietal Cluster', colormaps['Frontoparietal Cluster'])\n",
    "plot_cluster_topomap(central_mask, 'Central Cluster', colormaps['Central Cluster'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the cognitive tests\n",
    "demo_data['CWIT'] = demo_data[['cw_1', 'cw_2', 'cw_3', 'cw_4']].mean(axis=1)\n",
    "demo_data['WAIS_DST'] = demo_data['ds_tot']\n",
    "demo_data['RAVLT'] = demo_data[['ravlt_tot','ravlt_imm','ravlt_del']].mean(axis=1)\n",
    "demo_data['DKEFS'] = demo_data[['tmt_2', 'tmt_3', 'tmt_4']].mean(axis=1)\n",
    "demo_data['VFT'] = demo_data[['vf_1', 'vf_2', 'vf_3']].mean(axis=1)\n",
    "\n",
    "combined_data = demo_data.copy()\n",
    "\n",
    "combined_data['CWIT'] = replace_nans(combined_data['CWIT'])\n",
    "combined_data['WAIS_DST'] = replace_nans(combined_data['WAIS_DST'])\n",
    "combined_data['RAVLT'] = replace_nans(combined_data['RAVLT'])\n",
    "combined_data['DKEFS'] = replace_nans(combined_data['DKEFS'])\n",
    "combined_data['VFT'] = replace_nans(combined_data['VFT'])\n",
    "\n",
    "#general aperiodic factor data\n",
    "combined_data['exponent_average'] = replace_nans(np.array(results['exponent']).mean(axis=1))\n",
    "combined_data['offset_average'] = replace_nans(np.array(results['offset']).mean(axis=1))\n",
    "\n",
    "#cluster specific data\n",
    "combined_data['frontoparietal_exponent'] = replace_nans(frontoparietal_exponent)\n",
    "combined_data['central_exponent'] = replace_nans(central_exponent)\n",
    "combined_data['midline_exponent'] = replace_nans(midline_exponent)\n",
    "combined_data['occipital_exponent'] = replace_nans(occipital_exponent)\n",
    "combined_data['frontoparietal_offset'] = replace_nans(frontoparietal_offset)\n",
    "combined_data['central_offset'] = replace_nans(central_offset)\n",
    "combined_data['occipital_offset'] = replace_nans(occipital_offset)\n",
    "\n",
    "#remove any participants with aperiodic values that are 3 standard deviations from the mean\n",
    "combined_data = combined_data[(np.abs(combined_data['occipital_exponent'] - combined_data['occipital_exponent'].mean()) / combined_data['occipital_exponent'].std() < 3)]\n",
    "combined_data = combined_data[(np.abs(combined_data['frontoparietal_exponent'] - combined_data['frontoparietal_exponent'].mean()) / combined_data['frontoparietal_exponent'].std() < 3)]\n",
    "combined_data = combined_data[(np.abs(combined_data['midline_exponent'] - combined_data['midline_exponent'].mean()) / combined_data['midline_exponent'].std() < 3)]\n",
    "combined_data = combined_data[(np.abs(combined_data['central_exponent'] - combined_data['central_exponent'].mean()) / combined_data['central_exponent'].std() < 3)]\n",
    "combined_data = combined_data[(np.abs(combined_data['occipital_offset'] - combined_data['occipital_offset'].mean()) / combined_data['occipital_offset'].std() < 3)]\n",
    "combined_data = combined_data[(np.abs(combined_data['frontoparietal_offset'] - combined_data['frontoparietal_offset'].mean()) / combined_data['frontoparietal_offset'].std() < 3)]\n",
    "combined_data = combined_data[(np.abs(combined_data['central_offset'] - combined_data['central_offset'].mean()) / combined_data['central_offset'].std() < 3)]\n",
    "\n",
    "combined_data.to_csv('combined_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# Correlations and robust models. If working from the provided \"combined_data.csv\" file, start here.\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seperate the data by SexN\n",
    "male_data = combined_data[combined_data['SexN'] == 1]\n",
    "female_data = combined_data[combined_data['SexN'] == 0]\n",
    "\n",
    "# Plot two histograms of the age data\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))  # Adjusted the figure size for better display\n",
    "\n",
    "# Plot male data on the first axis\n",
    "sns.histplot(male_data['age'], kde=True, ax=axs[0], color='blue')\n",
    "axs[0].set_title(\"Male Age Distribution\", loc = 'left', fontsize=16, fontweight='bold', color='black', pad=20)\n",
    "axs[0].set_xlabel('Age', fontsize=16, color = 'black')\n",
    "axs[0].set_ylabel('Frequency', fontsize=16, color = 'black')\n",
    "\n",
    "\n",
    "#make the font size bigger\n",
    "\n",
    "# Plot female data on the second axis\n",
    "sns.histplot(female_data['age'], kde=True, ax=axs[1], color='red')\n",
    "axs[1].set_title(\"Female Age Distribution\", loc = 'left', fontsize=16, fontweight='bold', color='black', pad=20)\n",
    "axs[1].set_xlabel('Age', fontsize=16, color = 'black')\n",
    "axs[1].set_ylabel('Frequency', fontsize=16, color = 'black')\n",
    "\n",
    "#despine the plot\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "#Bivariate and partial correlations\n",
    "x_var = 'VFT'\n",
    "y_var = 'central_offset'\n",
    "\n",
    "sns.regplot(x=x_var, y=y_var, data=combined_data)\n",
    "pg.partial_corr(data=combined_data, x=x_var, y=y_var, covar='age', method='pearson')\n",
    "\n",
    "#Ordinary least squares model\n",
    "formula = f\"{y_var} ~ {x_var} + age + {x_var} * age\"\n",
    "model = smf.ols(formula, data=combined_data)\n",
    "results = model.fit()\n",
    "print(results.summary())\n",
    "\n",
    "#Robust model\n",
    "r_model = smf.rlm(formula, data=combined_data, M=sm.robust.norms.LeastSquares())\n",
    "r_results = r_model.fit()\n",
    "print(r_results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For calculating multiple comparisons corrected p-values for bivariate and partial correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select specific variables for correlation\n",
    "selected_vars = combined_data[[\n",
    "    \"age\",\n",
    "    \"exponent_average\",\n",
    "    \"offset_average\",\n",
    "    \"frontoparietal_exponent\",\n",
    "    \"central_exponent\",\n",
    "    \"midline_exponent\",\n",
    "    \"occipital_exponent\",\n",
    "    \"frontoparietal_offset\",\n",
    "    \"central_offset\",\n",
    "    \"occipital_offset\",\n",
    "    \"CWIT\",\n",
    "    \"WAIS_DST\",\n",
    "    \"RAVLT\",\n",
    "    \"DKEFS\",\n",
    "    \"VFT\"\n",
    "]]\n",
    "\n",
    "# Initialize a list to store results\n",
    "bivar_results = []\n",
    "\n",
    "# Calculate the bivariate correlation coefficients and p-values\n",
    "for i in range(len(selected_vars.columns) - 1):\n",
    "    for j in range(i + 1, len(selected_vars.columns)):\n",
    "        corr, p_value = pearsonr(selected_vars.iloc[:, i], selected_vars.iloc[:, j])\n",
    "        bivar_results.append({\n",
    "            'Var1': selected_vars.columns[i],\n",
    "            'Var2': selected_vars.columns[j],\n",
    "            'Correlation': corr,\n",
    "            'P.Value': p_value\n",
    "        })\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "bivar_results_df = pd.DataFrame(bivar_results)\n",
    "\n",
    "# Correct for multiple comparisons using FDR\n",
    "bivar_results_df['Corrected.P.Value'] = multipletests(bivar_results_df['P.Value'], method='fdr_bh')[1]\n",
    "bivar_results_df = pd.DataFrame(bivar_results_df)\n",
    "\n",
    "#save the bivar_results to a csv\n",
    "bivar_results_df.to_csv('correlation_results.csv', index=False)\n",
    "\n",
    "#Calculate the partial correlations controlling for age\n",
    "# Initialize a list to store results\n",
    "partial_corr_results = []\n",
    "\n",
    "# Select specific variables for correlation\n",
    "selected_vars = combined_data[[\n",
    "    \"exponent_average\",\n",
    "    \"offset_average\",\n",
    "    \"frontoparietal_exponent\",\n",
    "    \"central_exponent\",\n",
    "    \"midline_exponent\",\n",
    "    \"occipital_exponent\",\n",
    "    \"frontoparietal_offset\",\n",
    "    \"central_offset\",\n",
    "    \"occipital_offset\",\n",
    "    \"CWIT\",\n",
    "    \"WAIS_DST\",\n",
    "    \"RAVLT\",\n",
    "    \"DKEFS\",\n",
    "    \"VFT\"\n",
    "]]\n",
    "# Calculate the partial correlation coefficients and p-values\n",
    "for i in range(len(selected_vars.columns) - 1):\n",
    "    for j in range(i + 1, len(selected_vars.columns)):\n",
    "        partial_corr_res = partial_corr(\n",
    "            data=combined_data,\n",
    "            x=selected_vars.columns[i],\n",
    "            y=selected_vars.columns[j],\n",
    "            covar='age'\n",
    "        )\n",
    "        partial_corr_results.append({\n",
    "            'Var1': selected_vars.columns[i],\n",
    "            'Var2': selected_vars.columns[j],\n",
    "            'Partial.Correlation': partial_corr_res['r'][0],\n",
    "            'P.Value': partial_corr_res['p-val'][0]\n",
    "        })\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "partial_corr_results_df = pd.DataFrame(partial_corr_results)\n",
    "\n",
    "# Correct for multiple comparisons using FDR\n",
    "partial_corr_results_df['Corrected.P.Value'] = multipletests(partial_corr_results_df['P.Value'], method='fdr_bh')[1]\n",
    "partial_corr_results_df = pd.DataFrame(partial_corr_results_df)\n",
    "\n",
    "#save the partial_corr_results to a csv\n",
    "partial_corr_results_df.to_csv('partial_correlation_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For calculating multiple comparisons corrected p-values for robust regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from scipy.stats import false_discovery_control\n",
    "\n",
    "# Define your EEG variable and cognitive tests\n",
    "eeg_variable = 'occipital_offset'  # Specify your EEG variable\n",
    "cognitive_tests = ['CWIT', 'WAIS_DST', 'RAVLT', 'DKEFS', 'VFT']  # Replace with actual test names\n",
    "\n",
    "# Initialize separate lists for each type of p-values\n",
    "ap_activity_p_values = []  # Main effects of aperiodic activity\n",
    "age_p_values = []          # Main effects of age\n",
    "interaction_p_values = []  # Interaction effects\n",
    "\n",
    "# Loop over each cognitive test\n",
    "for test in cognitive_tests:\n",
    "    formula = f\"{test} ~ {eeg_variable} + age + {eeg_variable} * age\"\n",
    "    \n",
    "    # Robust linear model\n",
    "    model = smf.rlm(formula, data=combined_data)\n",
    "    results = model.fit()\n",
    "    \n",
    "    # Extract p-values for main effects and interaction term\n",
    "    ap_activity_p_values.append(results.pvalues[eeg_variable])  # Main effect of EEG variable\n",
    "    age_p_values.append(results.pvalues['age'])                 # Main effect of age\n",
    "    interaction_p_values.append(results.pvalues[f'{eeg_variable}:age'])  # Interaction term\n",
    "\n",
    "# Apply FDR correction separately for each category using scipy\n",
    "ap_activity_corrected = false_discovery_control(ap_activity_p_values, method='bh')\n",
    "age_corrected = false_discovery_control(age_p_values, method='bh')\n",
    "interaction_corrected = false_discovery_control(interaction_p_values, method='bh')\n",
    "\n",
    "# Print corrected p-values\n",
    "print(\"\\nCorrected Aperiodic Activity Main Effects p-values:\", ap_activity_corrected)\n",
    "print(\"\\nCorrected Age Main Effects p-values:\", age_corrected)\n",
    "print(\"\\nCorrected Interaction Effects p-values:\", interaction_corrected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# Simple slopes and Johnson-Neyman plots are conducted in R.\n",
    "# ============================================================================="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EEG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
